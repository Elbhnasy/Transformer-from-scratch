{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004d588f",
   "metadata": {},
   "source": [
    "# Transformer Attention Visualization\n",
    "\n",
    "This notebook visualizes the attention patterns in a trained Transformer model. It shows how different attention heads focus on different parts of the input sequence, helping understand the model's internal representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59693116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "from model import Transformer\n",
    "from config import get_config, get_weights_file_path, latest_weights_file_path\n",
    "from train import get_model, get_ds, greedy_decode\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4107da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device - use MPS for Mac M1/M2 if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                     \"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Set better defaults for GPU memory management if using CUDA\n",
    "if device.type == 'cuda':\n",
    "    # Free up GPU memory before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f216d1",
   "metadata": {},
   "source": [
    "## Model Loading and Setup\n",
    "\n",
    "Load the trained model and prepare for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and prepare datasets\n",
    "start_time = time.time()\n",
    "print(\"Loading configuration and datasets...\")\n",
    "\n",
    "try:\n",
    "    # Load configuration\n",
    "    config = get_config()\n",
    "    \n",
    "    # Get datasets and tokenizers\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    print(f\"Loaded datasets in {time.time() - start_time:.2f}s\")\n",
    "    print(f\"Source vocabulary size: {tokenizer_src.get_vocab_size():,}\")\n",
    "    print(f\"Target vocabulary size: {tokenizer_tgt.get_vocab_size():,}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    \n",
    "    # Count model parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model has {total_params:,} parameters\")\n",
    "    \n",
    "    # Load the pretrained weights with better error handling\n",
    "    model_filename = get_weights_file_path(config, \"29\")\n",
    "    \n",
    "    if not Path(model_filename).exists():\n",
    "        print(f\"Warning: Weight file not found at {model_filename}\")\n",
    "        print(\"Trying to find latest weights instead...\")\n",
    "        model_filename = latest_weights_file_path(config)\n",
    "        if not model_filename:\n",
    "            raise FileNotFoundError(\"No model weights found!\")\n",
    "    \n",
    "    print(f\"Loading weights from {model_filename}\")\n",
    "    # Load weights directly to the target device for better memory usage\n",
    "    state = torch.load(model_filename, map_location=device)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    print(f\"Loaded weights from epoch {state.get('epoch', 'unknown')}\")\n",
    "    \n",
    "    # Set model to evaluation mode for visualization\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model preparation completed in {time.time() - start_time:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model setup: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa6cd0",
   "metadata": {},
   "source": [
    "## Data Preparation for Visualization\n",
    "\n",
    "Prepare sample data for the attention visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27edaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_next_batch():\n",
    "    \"\"\"Load a batch from validation set and prepare for attention visualization.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (batch data, encoder tokens, decoder tokens)\n",
    "    \"\"\"\n",
    "    # Load a sample batch from the validation set\n",
    "    try:\n",
    "        batch = next(iter(val_dataloader))\n",
    "        encoder_input = batch[\"encoder_input\"].to(device)\n",
    "        encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "        decoder_input = batch[\"decoder_input\"].to(device)\n",
    "        decoder_mask = batch[\"decoder_mask\"].to(device)\n",
    "\n",
    "        # Verify batch size is 1 for visualization\n",
    "        assert encoder_input.size(0) == 1, \"Batch size must be 1 for visualization\"\n",
    "        \n",
    "        # Convert tokens outside of loop for performance\n",
    "        encoder_input_cpu = encoder_input[0].cpu().numpy()\n",
    "        decoder_input_cpu = decoder_input[0].cpu().numpy()\n",
    "        \n",
    "        # Safely convert token IDs to tokens with safeguards for missing IDs\n",
    "        encoder_input_tokens = []\n",
    "        for idx in encoder_input_cpu:\n",
    "            try:\n",
    "                token = tokenizer_src.id_to_token(int(idx))\n",
    "                encoder_input_tokens.append(token if token else \"[UNK]\")\n",
    "            except:\n",
    "                encoder_input_tokens.append(\"[UNK]\")\n",
    "        \n",
    "        decoder_input_tokens = []\n",
    "        for idx in decoder_input_cpu:\n",
    "            try:\n",
    "                token = tokenizer_tgt.id_to_token(int(idx))\n",
    "                decoder_input_tokens.append(token if token else \"[UNK]\")\n",
    "            except:\n",
    "                decoder_input_tokens.append(\"[UNK]\")\n",
    "\n",
    "        # Run the model to generate and capture attention\n",
    "        with torch.no_grad():\n",
    "            model_out = greedy_decode(\n",
    "                model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, \n",
    "                config['seq_len'], device\n",
    "            )\n",
    "        \n",
    "        return batch, encoder_input_tokens, decoder_input_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading batch: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadf455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions for attention maps\n",
    "def mtx2df(m, max_row, max_col, row_tokens, col_tokens):\n",
    "    \"\"\"Convert attention matrix to DataFrame for visualization.\n",
    "    \n",
    "    Args:\n",
    "        m: Attention matrix\n",
    "        max_row: Maximum number of rows to include\n",
    "        max_col: Maximum number of columns to include\n",
    "        row_tokens: Tokens for rows\n",
    "        col_tokens: Tokens for columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Formatted data for Altair visualization\n",
    "    \"\"\"\n",
    "    # Pre-allocate list with estimated size\n",
    "    estimated_size = min(max_row, m.shape[0]) * min(max_col, m.shape[1])\n",
    "    data = []\n",
    "    \n",
    "    # Faster iteration without nested list comprehension\n",
    "    for r in range(min(max_row, m.shape[0])):\n",
    "        for c in range(min(max_col, m.shape[1])):\n",
    "            # Get token labels with safeguards\n",
    "            row_token = row_tokens[r] if r < len(row_tokens) else \"<blank>\"\n",
    "            col_token = col_tokens[c] if c < len(col_tokens) else \"<blank>\"\n",
    "            \n",
    "            data.append((\n",
    "                r,\n",
    "                c,\n",
    "                float(m[r, c]),  # Convert to Python float for better compatibility\n",
    "                f\"{r:03d} {row_token}\",\n",
    "                f\"{c:03d} {col_token}\"\n",
    "            ))\n",
    "    \n",
    "    return pd.DataFrame(\n",
    "        data,\n",
    "        columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"]\n",
    "    )\n",
    "\n",
    "def get_attn_map(attn_type: str, layer: int, head: int):\n",
    "    \"\"\"Extract attention scores from the specified attention mechanism.\n",
    "    \n",
    "    Args:\n",
    "        attn_type: Type of attention (\"encoder\", \"decoder\", or \"encoder-decoder\")\n",
    "        layer: Layer index\n",
    "        head: Attention head index\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Attention scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if attn_type == \"encoder\":\n",
    "            attn = model.encoder.layers[layer].self_attention_block.attention_scores\n",
    "        elif attn_type == \"decoder\":\n",
    "            attn = model.decoder.layers[layer].self_attention_block.attention_scores\n",
    "        elif attn_type == \"encoder-decoder\":\n",
    "            attn = model.decoder.layers[layer].cross_attention_block.attention_scores\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown attention type: {attn_type}\")\n",
    "            \n",
    "        # Clone and detach to avoid memory leaks\n",
    "        return attn[0, head].clone().detach().cpu()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting attention map for {attn_type}, layer {layer}, head {head}: {e}\")\n",
    "        # Return an empty tensor as fallback\n",
    "        return torch.zeros((1, 1))\n",
    "\n",
    "def attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len):\n",
    "    \"\"\"Create an Altair chart for an attention map.\n",
    "    \n",
    "    Args:\n",
    "        attn_type: Type of attention\n",
    "        layer: Layer index\n",
    "        head: Head index\n",
    "        row_tokens: Tokens for rows\n",
    "        col_tokens: Tokens for columns\n",
    "        max_sentence_len: Maximum sentence length to display\n",
    "        \n",
    "    Returns:\n",
    "        alt.Chart: Visualization of attention weights\n",
    "    \"\"\"\n",
    "    # Get the attention map data\n",
    "    attn_data = get_attn_map(attn_type, layer, head)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = mtx2df(\n",
    "        attn_data,\n",
    "        max_sentence_len,\n",
    "        max_sentence_len,\n",
    "        row_tokens,\n",
    "        col_tokens,\n",
    "    )\n",
    "    \n",
    "    # Drop NaN values that might cause rendering issues\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Create the heatmap visualization\n",
    "    chart = alt.Chart(data=df).mark_rect().encode(\n",
    "        x=alt.X(\"col_token:N\", axis=alt.Axis(title=\"\", labelAngle=-45)),\n",
    "        y=alt.Y(\"row_token:N\", axis=alt.Axis(title=\"\")),\n",
    "        color=alt.Color(\"value:Q\", scale=alt.Scale(scheme=\"viridis\")),\n",
    "        tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "    ).properties(\n",
    "        height=400, \n",
    "        width=400, \n",
    "        title=f\"{attn_type.capitalize()} Layer {layer} Head {head}\"\n",
    "    ).interactive()\n",
    "    \n",
    "    return chart\n",
    "\n",
    "def get_all_attention_maps(attn_type: str, layers: list[int], heads: list[int], \n",
    "                           row_tokens: list, col_tokens: list, max_sentence_len: int):\n",
    "    \"\"\"Generate a grid of attention map visualizations.\n",
    "    \n",
    "    Args:\n",
    "        attn_type: Type of attention\n",
    "        layers: List of layer indices\n",
    "        heads: List of head indices\n",
    "        row_tokens: Tokens for rows\n",
    "        col_tokens: Tokens for columns\n",
    "        max_sentence_len: Maximum sentence length to display\n",
    "        \n",
    "    Returns:\n",
    "        alt.VConcatChart: Grid of attention visualizations\n",
    "    \"\"\"\n",
    "    # Show progress\n",
    "    print(f\"Generating {len(layers) * len(heads)} attention maps for {attn_type}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    charts = []\n",
    "    for i, layer in enumerate(layers):\n",
    "        row_charts = []\n",
    "        for head in heads:\n",
    "            # Create chart for this layer and head\n",
    "            row_charts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len))\n",
    "            \n",
    "        # Combine charts for this layer horizontally\n",
    "        charts.append(alt.hconcat(*row_charts, spacing=5))\n",
    "        \n",
    "        # Show progress for long operations\n",
    "        if (i+1) % 2 == 0:\n",
    "            print(f\"Processed {i+1}/{len(layers)} layers...\")\n",
    "    \n",
    "    # Combine all layers vertically\n",
    "    result = alt.vconcat(*charts, spacing=10)\n",
    "    print(f\"Completed in {time.time() - start_time:.2f}s\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca08114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare sample data for visualization\n",
    "try:\n",
    "    batch, encoder_input_tokens, decoder_input_tokens = load_next_batch()\n",
    "    \n",
    "    # Display source and target text\n",
    "    print(f\"Source: {batch['src_text'][0]}\")\n",
    "    print(f\"Target: {batch['tgt_text'][0]}\")\n",
    "    \n",
    "    # Find the end of the actual content (before padding)\n",
    "    try:\n",
    "        sentence_len = encoder_input_tokens.index(\"[PAD]\")\n",
    "        print(f\"Sentence length (before padding): {sentence_len}\")\n",
    "    except ValueError:\n",
    "        # Fallback if [PAD] not found - use a reasonable length\n",
    "        sentence_len = min(20, len(encoder_input_tokens))\n",
    "        print(f\"[PAD] token not found. Using length: {sentence_len}\")\n",
    "    \n",
    "    # Display token sequences for debugging\n",
    "    print(\"\\nSource tokens (first 20):\")\n",
    "    print(encoder_input_tokens[:min(20, len(encoder_input_tokens))])\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing sample: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb49178",
   "metadata": {},
   "source": [
    "## Attention Visualizations\n",
    "\n",
    "Generate visualizations for the three types of attention mechanisms in the transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for visualization\n",
    "layers = [0, 1, 2]  # Use more layers if needed\n",
    "heads = [0, 1, 2, 3, 4, 5, 6, 7]  # 8 attention heads\n",
    "\n",
    "# Limit sentence length for clearer visualization\n",
    "vis_len = min(20, sentence_len)\n",
    "\n",
    "print(f\"Visualizing attention for first {vis_len} tokens across {len(layers)} layers and {len(heads)} heads\")\n",
    "\n",
    "# 1. Encoder Self-Attention: How tokens in the encoder attend to each other\n",
    "get_all_attention_maps(\"encoder\", layers, heads, encoder_input_tokens, encoder_input_tokens, vis_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b63e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Decoder Self-Attention: How tokens in the decoder attend to previous tokens\n",
    "get_all_attention_maps(\"decoder\", layers, heads, decoder_input_tokens, decoder_input_tokens, vis_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a556abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Cross-Attention: How decoder tokens attend to encoder tokens\n",
    "get_all_attention_maps(\"encoder-decoder\", layers, heads, decoder_input_tokens, encoder_input_tokens, vis_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb58f12",
   "metadata": {},
   "source": [
    "## Attention Pattern Analysis\n",
    "\n",
    "The visualizations above show different patterns in the attention mechanisms:\n",
    "\n",
    "1. **Encoder Self-Attention**: Shows how words in the source sentence relate to each other. Look for patterns where:\n",
    "   - Words attend to related words or context\n",
    "   - Some heads focus on local relationships (nearby words)\n",
    "   - Others capture long-range dependencies\n",
    "\n",
    "2. **Decoder Self-Attention**: Shows how words in the target attend to previous words (causal attention). Notice:\n",
    "   - The triangular pattern (words can only see previous words)\n",
    "   - Different roles for different heads\n",
    "\n",
    "3. **Cross-Attention**: Shows how target words look at source words. This reveals:\n",
    "   - Word alignment between languages\n",
    "   - Which source words are most important for translating each target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee921224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory - important for notebook environments, especially with GPU\n",
    "import gc\n",
    "\n",
    "# Remove references to large objects\n",
    "del model\n",
    "del train_dataloader\n",
    "del val_dataloader\n",
    "\n",
    "# Clean up CUDA memory if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
