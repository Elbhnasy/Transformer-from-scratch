# Transformer Implementation from Scratch

This repository contains a clean, from-scratch implementation of the Transformer architecture, as introduced in the seminal paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) by Vaswani et al. The goal is to build a deep understanding of the model by implementing each component manually without relying on high-level deep learning libraries.

## Features
- Encoder and decoder stacks
- Scaled dot-product attention and multi-head attention
- Positional encoding
- Masking mechanisms
- Fully connected layers with layer normalization and dropout
- PyTorch-based, but avoids using `nn.Transformer` or similar high-level modules

## Installation & Usage
Coming soon...

## License
MIT
